## 传统Seq2Seq
我们来举一个**传统的基于RNN的Seq2Seq模型**在**机器翻译**任务中的例子。(LSTM同理)

### 例子：中文到英文的机器翻译

假设我们要将中文句子“我爱吃苹果”翻译成英文“I love to eat apples”。

#### 1\. 模型组成

  * **编码器（Encoder）**: 一个RNN（比如LSTM或GRU），负责读取输入中文句子。
  * **解码器（Decoder）**: 另一个RNN（比如LSTM或GRU），负责生成输出英文句子。

#### 2\. 编码器的工作流程

1.  **输入**: 编码器按顺序接收中文句子的每个词的词向量：

      * `<start>` (特殊的起始标记)
      * “我”
      * “爱”
      * “吃”
      * “苹果”
      * `<end>` (特殊的结束标记)

2.  **处理**: 编码器RNN的每个时间步都会处理一个输入词，并更新其内部的**隐藏状态**。

3.  **最终上下文向量**: 当编码器处理完整个输入序列（即读到 `<end>` 标记）后，它的**最终隐藏状态**（或称为细胞状态和隐藏状态的组合）被作为整个输入句子的**固定长度的上下文向量（Context Vector）**。这个上下文向量被认为是包含了整个中文句子语义信息的压缩表示。

      * **核心思想**: 编码器试图将整个输入序列的复杂含义“压缩”到这一个向量中。

#### 3\. 解码器的工作流程

1.  **初始化**: 解码器接收编码器生成的**上下文向量**作为其初始隐藏状态。
2.  **生成第一个词**:
      * 解码器以特殊的起始标记 `<start>` 作为第一个输入。
      * 结合其当前隐藏状态（也就是编码器传来的上下文向量），解码器RNN计算并输出下一个隐藏状态。
      * 然后，通过一个全连接层和Softmax函数，预测输出词的概率分布。
      * 模型选择概率最高的词作为第一个输出词，例如：“I”。
3.  **迭代生成后续词**:
      * 将上一步生成的词（例如“I”）作为当前时间步的输入。

      * 解码器RNN利用上一步的隐藏状态和当前的输入词，计算新的隐藏状态，并预测下一个输出词。

      * 这个过程重复进行，直到解码器预测到特殊的结束标记 `<end>`（表示句子生成完毕），或者达到预设的最大长度。

      * 例如，它会依次生成：“love”、“to”、“eat”、“apples”、“\<end\>”。

#### 4\. 传统模型的局限性

在这个例子中，**所有的信息都必须通过那个单一的、固定长度的上下文向量传递**。

  * **长句问题**: 如果中文句子非常长（比如几十个词），这个固定长度的上下文向量就很难有效地编码所有重要的信息。它就像一个\*\*“信息瓶颈”\*\*。重要的细节可能会在压缩过程中丢失，导致解码器难以准确地生成长句的翻译。
  * **信息丢失**: 随着输入序列的增长，编码器就越难将所有相关信息“塞进”那个固定大小的向量。这就好比试图用一个非常小的U盘存储一部超高清电影，总会丢失很多细节。
  * **对齐困难**: 模型很难知道输出句子中的某个词（比如“apples”）具体对应输入句子中的哪个词（“苹果”），因为它只有一个全局的上下文信息。


### 为什么需要注意力机制？

在引入注意力机制之前，传统的Seq2Seq模型使用一个**固定长度的上下文向量**来承载编码器对整个输入序列的表示。这个向量被称为**“瓶颈”**，因为它限制了编码器能够传递给解码器的信息量。当输入序列很长时，这个固定长度的向量很难有效地编码所有相关信息，导致模型性能下降。

注意力机制解决了这个问题，它允许解码器在每一步输出时，**选择性地关注输入序列中与当前输出最相关的部分**。这就像人类在翻译一句话时，会根据正在翻译的词语，回过头去查看原文中对应的部分一样。

### Bahdanau 注意力机制的工作原理

Bahdanau 注意力机制的核心思想是计算**对齐分数（alignment scores）**，这些分数表示输入序列中每个编码器隐藏状态与当前解码器隐藏状态之间的相关性。这些分数经过归一化后，形成一个**注意力权重（attention weights）**分布，然后用这些权重对编码器隐藏状态进行加权平均，得到一个**上下文向量（context vector）**。

下面是其关键步骤的详细解释：

1.  **解码器隐藏状态 ($s_{i-1}$)**：在生成第 $i$ 个输出词之前，解码器会有一个当前的隐藏状态 $s_{i-1}$。这个状态包含了解码器到目前为止已经处理过的信息。

2.  **编码器隐藏状态 ($h_j$)**：编码器在处理输入序列时，会为每个输入词生成一个隐藏状态 $h_j$。这些 $h_j$ 包含了输入序列中每个位置的信息。

3.  **计算对齐分数 ($e_{ij}$)**：这是 Bahdanau 注意力机制的关键步骤。它使用一个**前馈神经网络（feed-forward neural network）**来计算解码器当前隐藏状态 $s_{i-1}$ 和每个编码器隐藏状态 $h_j$ 之间的“兼容性”或“相关性”：

    $$e_{ij} = \mathbf{v}_a^\top \tanh(\mathbf{W}_a s_{i-1} + \mathbf{U}_a h_j)$$

    * $\mathbf{v}_a$, $\mathbf{W}_a$, $\mathbf{U}_a$ 是可学习的权重参数。
    * $\tanh$ 是一个非线性激活函数。
    * 这个前馈神经网络被称为**对齐模型（alignment model）**。它的输出 $e_{ij}$ 代表了在生成第 $i$ 个输出词时，编码器第 $j$ 个隐藏状态对解码器当前状态的重要性。

4.  **计算注意力权重 ($\alpha_{ij}$)**：为了将对齐分数转换为概率分布，我们对 $e_{ij}$ 进行 **softmax 归一化**：

    $$\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{L_x} \exp(e_{ik})}$$

    * $L_x$ 是输入序列的长度。
    * $\alpha_{ij}$ 表示在生成第 $i$ 个输出词时，对编码器第 $j$ 个隐藏状态的注意力权重。所有 $\alpha_{ij}$ 的和为 1。

5.  **计算上下文向量 ($c_i$)**：使用注意力权重对编码器隐藏状态进行加权平均，得到当前时间步的上下文向量：

    $$c_i = \sum_{j=1}^{L_x} \alpha_{ij} h_j$$

    * 这个上下文向量 $c_i$ 包含了输入序列中与当前解码器输出最相关的信息。

6.  **更新解码器隐藏状态并预测输出**：将上下文向量 $c_i$ 与解码器当前隐藏状态 $s_{i-1}$ 结合（通常是拼接），然后输入到解码器的 RNN 单元中，更新解码器隐藏状态 $s_i$，并最终预测第 $i$ 个输出词。

### Bahdanau 注意力机制的优势

* **克服了固定上下文向量的瓶颈**：允许模型处理长序列，并捕获输入序列中更丰富的信息。
* **提高了模型性能**：在机器翻译等任务中显著提升了翻译质量。
* **提供了可解释性**：注意力权重可以被可视化，显示模型在生成某个输出词时“关注”了输入序列的哪些部分，从而提供了一定的可解释性。
* **允许模型处理输入序列中的长距离依赖**：即使两个相关的词在输入序列中相距很远，注意力机制也能帮助模型建立它们之间的联系。

### 总结

经典的 Bahdanau 注意力机制通过引入一个**可学习的对齐模型**，动态地计算解码器在生成每个输出词时对输入序列不同部分的关注程度。它通过加权平均编码器隐藏状态来生成上下文向量，从而克服了传统 Seq2Seq 模型中固定上下文向量的局限性，极大地提升了模型的表现和可解释性。

