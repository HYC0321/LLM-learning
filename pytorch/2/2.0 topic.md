目标：理解 torch.autograd 机制，并能手动完成一次梯度计算和参数更新。


理解 torch.autograd 机制
torch.autograd 是 PyTorch 的自动微分引擎。它能够自动计算张量上的所有操作的梯度。这是深度学习训练能够实现的关键，因为它允许我们高效地计算损失函数相对于模型参数的梯度，进而使用梯度下降等优化算法来更新参数。
核心概念
1. 
计算图 (Computation Graph)
当你对 PyTorch 张量进行操作时（比如加法、乘法、矩阵乘法等），autograd 会在后台默默地构建一个动态计算图。这个图记录了所有这些操作，以及它们之间的依赖关系。
• 节点 (Nodes): 张量 (Tensors) 是图中的节点。
• 边 (Edges): 对张量执行的操作 (Operations) 是图中的边。
• 叶子节点 (Leaf Nodes): 通常是那些由用户直接创建的、需要计算梯度的张量（比如模型的权重和偏置）。
2. 
requires_grad 属性
每个张量都有一个 requires_grad 属性。
• 如果一个张量设置为 requires_grad=True，那么 PyTorch 会追踪对它进行的所有操作，并在计算图上记录下来，以便后续计算梯度。
• 默认情况下，用户直接创建的张量（如 torch.tensor）的 requires_grad 是 False。
• 模型参数（如 nn.Linear 中的 weight 和 bias）默认就是 requires_grad=True。
• 如果一个张量是某个操作的结果，并且这个操作的至少一个输入张量的 requires_grad=True，那么结果张量的 requires_grad 也会是 True。
3. 
grad 属性
如果一个张量 x 的 requires_grad 是 True，并且它参与了梯度计算（即在 loss.backward() 被调用后），那么它的梯度会累积存储在 x.grad 属性中。
4. 
grad_fn 属性
如果一个张量是通过某个操作创建的（而不是用户直接创建的），那么它会有一个 grad_fn 属性。这个属性指向一个函数，该函数知道如何计算相对于其输入张量的梯度。它代表了计算图中的一个“边”或操作。
5. 
backward() 方法
这是梯度计算的“触发器”。通常，你会在标量损失张量（例如你的模型的输出与真实标签之间的误差）上调用 loss.backward()。
• 调用 backward() 会沿着计算图反向遍历，从损失张量开始，递归地计算所有 requires_grad=True 的叶子节点的梯度，并将其累加到各自的 .grad 属性中。
• 注意：backward() 只能在标量张量上调用，或者如果你在一个非标量张量上调用，你需要传入一个 gradient 参数，它是一个与非标量张量形状相同的张量，表示“外部梯度”。 在深度学习中，通常损失是标量，所以我们直接调用 loss.backward()。
梯度累积
PyTorch 的梯度是累积的。这意味着每次调用 loss.backward() 时，新的梯度会加到现有 .grad 属性的值上。
• 因此，在每次梯度计算之前，你通常需要使用 optimizer.zero_grad() 或手动 tensor.grad.zero_() 来清零之前的梯度，防止它们对当前步的计算造成干扰。
￼
PyTorch autograd 练习
练习 1：基础梯度计算
目标： 理解 requires_grad 的作用，并进行一次简单的梯度计算。
1. 创建张量并设置 requires_grad：
• 创建一个张量 x，值为 ，并设置 requires_grad=True。
• 创建一个张量 y，值为 ，不设置 requires_grad。
• 创建一个张量 z，值为 ，并设置 requires_grad=True。
2. 执行操作构建计算图：
• 计算 a = x * y。
• 计算 b = a + z。
• 计算 c = b.sum() (确保 c 是一个标量)。
3. 检查属性：
• 打印 a.grad_fn, b.grad_fn, c.grad_fn。
• 打印 x.requires_grad, y.requires_grad, z.requires_grad, a.requires_grad, b.requires_grad, c.requires_grad。
4. 执行反向传播：
• 在 c 上调用 backward()。
• 打印 x.grad, y.grad, z.grad。解释为什么有些张量的梯度是 None。
￼
练习 2：手动梯度清零与累积
目标： 演示梯度累积行为，并学会手动清零梯度。
1. 准备张量：
• 创建一个张量 data，值为 ，requires_grad=True。
• 创建一个张量 weight，值为 ，requires_grad=True。
• 创建一个张量 bias，值为 ，requires_grad=True。
2. 第一次前向传播与反向传播：
• 计算 output1 = data * weight + bias。
• 计算 loss1 = output1.sum()。
• 对 loss1 调用 backward()。
• 打印 data.grad, weight.grad, bias.grad。
3. 第二次前向传播与反向传播 (不清零)：
• 不进行任何梯度清零操作。
• 再次计算 output2 = data * weight + bias。
• 计算 loss2 = output2.sum()。
• 对 loss2 调用 backward()。
• 再次打印 data.grad, weight.grad, bias.grad。观察梯度是如何变化的。
4. 手动清零梯度：
• 将 data.grad, weight.grad, bias.grad 清零（例如使用 .zero_() 方法）。
• 再次打印它们的梯度，确认已清零。
5. 第三次前向传播与反向传播 (清零后)：
• 再次计算 output3 = data * weight + bias。
• 计算 loss3 = output3.sum()。
• 对 loss3 调用 backward()。
• 再次打印 data.grad, weight.grad, bias.grad。观察这次的梯度。
￼
练习 3：手动参数更新（模拟一步梯度下降）
目标： 理解如何利用梯度来手动更新模型参数。
我们将模拟一个非常简单的线性回归模型：。
1. 定义数据和真实参数：
• 创建一个  的张量 x_data，包含一些随机数（例如 torch.randn(10, 1)）。
• 定义一个真实的权重 true_w（例如 ）。
• 定义一个真实的偏置 true_b（例如 ）。
• 计算真实的标签 y_true = true_w * x_data + true_b。
2. 定义可学习参数：
• 创建一个张量 w，初始值为一个随机数（例如 torch.randn(1, 1)），并设置 requires_grad=True。
• 创建一个张量 b，初始值为一个随机数（例如 torch.randn(1, 1)），并设置 requires_grad=True。
3. 设置学习率和迭代次数：
• learning_rate = 0.01
• num_iterations = 100
4. 训练循环（手动）：
• 在一个 for 循环中迭代 num_iterations 次：
• 前向传播： 计算预测值 y_pred = w * x_data + b。
• 计算损失： 使用均方误差 (MSE) 作为损失函数：loss = torch.mean((y_pred - y_true)**2)。
• 梯度清零： 在反向传播之前，清零 w.grad 和 b.grad。
• 反向传播： 对 loss 调用 backward()。
• 参数更新：
• 更新 w: w.data = w.data - learning_rate * w.grad.data
• 更新 b: b.data = b.data - learning_rate * b.grad.data
• 为什么使用 .data？ 在更新参数时，我们通常会使用 .data 属性。这是为了在更新过程中将操作从计算图中剥离，避免 PyTorch 尝试计算更新操作的梯度（这通常不是我们想要的）。另一种更现代和推荐的方法是使用 with torch.no_grad(): 上下文管理器来包裹更新操作。
• 打印： 每隔  次迭代，打印当前的 loss 值以及 w 和 b 的值。
5. 观察结果：
• 训练结束后，w 和 b 的值应该会非常接近 true_w 和 true_b。
￼
额外挑战：使用 torch.no_grad()
目标： 演示如何在不影响计算图的情况下执行操作，以及在参数更新中使用 torch.no_grad()。
1. 重做练习 3 的参数更新部分：
• 将 w.data = w.data - learning_rate * w.grad.data 替换为：
Python
Copy code
with
 torch.no_grad():
    w -= learning_rate * w.grad
    b -= learning_rate * b.grad
• 解释为什么 w -= ... 而不是 w = w - ...。(-= 是原地操作，会直接修改张量内容，而 = w - ... 会创建一个新的张量并赋值给 w)。
• 比较两种更新方式（使用 .data 和 with torch.no_grad()）在实际模型训练中的优缺点。
￼
希望这些练习能让你对 PyTorch 的 autograd 机制有更深刻的理解！ 开始动手尝试吧，通过实践来巩固这些核心概念。