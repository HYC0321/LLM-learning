残差连接（Residual Connection），也称为跳跃连接（Skip Connection），是深度学习中一种强大的技术，它通过在神经网络中创建“捷径”来允许信息跳过一个或多个层，从而彻底改变了深度神经网络的训练方式。
## 核心思想
在传统的深度神经网络中，信息是按顺序逐层传递的。每一层的输出都作为下一层的输入。然而，随着网络深度的增加，会出现两个主要问题：

* 梯度消失/爆炸： 在反向传播过程中，梯度在逐层传递时可能会变得非常小（梯度消失）或非常大（梯度爆炸），这使得网络难以有效训练。
* 网络退化： 简单地增加网络深度，当达到一定程度后，模型的性能反而会下降，即训练误差和测试误差都会增加。

残差连接通过引入一个简单的加法操作来解决这些问题。假设一个或多个层的输入是 x，这些层学习的函数是 F(x)。在传统的网络中，输出就是 F(x)。而在具有残差连接的网络中，输出变成了 H(x) = F(x) + x。 这里的 x 就是“跳跃”过来的原始输入，它创建了一条信息和梯度流动的直接通道。

## 残差连接为何有效？
残差连接的有效性主要体现在以下几个方面：
* 缓解梯度消失问题： 恒等连接（即 +x 的部分）确保了即使在网络很深的情况下，梯度也能够直接回传到较浅的层，从而防止梯度在反向传播过程中消失。
* 简化学习过程： 网络不再需要从头学习一个完整的映射关系，而是学习输入与期望输出之间的“残差”F(x)。如果理想的映射关系接近于恒等映射（即输出应该和输入差不多），那么网络只需要将 F(x) 的权重学习到接近于零即可，这比学习一个复杂的非线性变换要容易得多。
* 解决网络退化问题： 通过残差连接，增加网络深度至少不会让模型性能变差。在最坏的情况下，新添加的层只需学习一个恒等映射（即让 F(x)=0），网络性能就等同于较浅的网络。
* 促进特征重用： 允许深层网络直接访问浅层网络的特征，有助于保留和优化在不同处理阶段提取到的重要特征。
* 平滑损失函数曲面： 研究表明，残差连接可以使损失函数的曲面变得更加平滑，这有助于网络的优化和收敛。