![alt text](image.png)

### 1. **ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆScaled Dot-Product Attentionï¼‰**
å•å¤´æ³¨æ„åŠ›çš„è®¡ç®—æ–¹å¼ï¼š
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]
- \( Q \)ï¼ˆQueryï¼‰ã€\( K \)ï¼ˆKeyï¼‰ã€\( V \)ï¼ˆValueï¼‰æ˜¯è¾“å…¥çŸ©é˜µã€‚
- \( d_k \) æ˜¯Keyçš„ç»´åº¦ï¼Œç¼©æ”¾å› å­ \(\sqrt{d_k}\) ç”¨äºé˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚

---

### 2. **å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰**
å°†Queryã€Keyã€Valueé€šè¿‡ä¸åŒçš„çº¿æ€§æŠ•å½±æ‹†åˆ†æˆ \( h \) ä¸ªå¤´ï¼Œç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›åæ‹¼æ¥ï¼š
\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
\]
å…¶ä¸­æ¯ä¸ªå¤´çš„è®¡ç®—ä¸ºï¼š
\[
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\]
- \( W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k} \), \( W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k} \), \( W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v} \) æ˜¯ç¬¬ \( i \) ä¸ªå¤´çš„æŠ•å½±çŸ©é˜µã€‚
- \( W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}} \) æ˜¯è¾“å‡ºæŠ•å½±çŸ©é˜µã€‚
- è®ºæ–‡ä¸­é»˜è®¤ \( h=8 \) ä¸ªå¤´ï¼Œä¸” \( d_k = d_v = d_{\text{model}}/h = 64 \)ï¼ˆå½“ \( d_{\text{model}}=512 \) æ—¶ï¼‰ã€‚

---

å¤šå¤´æ³¨æ„åŠ›èƒ½è®©æ¨¡å‹ä»å¤šä¸ªå­ç©ºé—´ã€å¤šä¸ªè§’åº¦å¹¶è¡Œåœ°å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼Œ
æ•æ‰æ›´ä¸°å¯Œã€æ›´ç»†è‡´çš„è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯ã€‚

---

## å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„**å®Œæ•´è®¡ç®—æµç¨‹**



## ğŸ§  åŸºç¡€è®¾ç½®ï¼š

| å‚æ•°å                | å«ä¹‰               | ç¤ºä¾‹æ•°å€¼                   |
| ------------------ | ---------------- | ---------------------- |
| $B$                | Batch size       | 32                     |
| $T$                | åºåˆ—é•¿åº¦ï¼ˆtokenæ•°ï¼‰     | 20                     |
| $d_{\text{model}}$ | æ¨¡å‹ç»´åº¦             | 512                    |
| $h$                | å¤´çš„æ•°é‡ï¼ˆnum\_headsï¼‰ | 8                      |
| $d_k = d_v$        | æ¯ä¸ªå¤´çš„ç»´åº¦           | 64 ï¼ˆå› ä¸º $512 / 8 = 64$ï¼‰ |

---

## âœ… Step 1ï¼šè¾“å…¥

$$
X \in \mathbb{R}^{B \times T \times d_{\text{model}}}
$$

* X æ˜¯æ¯ä¸ª token çš„ embedding æˆ–ä¸Šå±‚è¾“å‡ºã€‚

---

## âœ… Step 2ï¼šçº¿æ€§å˜æ¢å¾—åˆ° Q, K, V

ä½¿ç”¨ç»Ÿä¸€çš„çº¿æ€§å±‚å¾—åˆ° 3 ä¸ªçŸ©é˜µï¼š

$$
Q = XW^Q,\quad K = XW^K,\quad V = XW^V
$$

æƒé‡çŸ©é˜µå½¢çŠ¶ï¼š

* $W^Q, W^K, W^V \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$

å› æ­¤è¾“å‡ºå½¢çŠ¶ï¼š

$$
Q, K, V \in \mathbb{R}^{B \times T \times d_{\text{model}}}
$$

---

## âœ… Step 3ï¼šreshape æ‹†æˆå¤šä¸ªå¤´

æˆ‘ä»¬å°† Q/K/V reshape æˆå¤šä¸ª attention å¤´ï¼š

```python
Q â†’ [B, T, h, d_k] â†’ [B, h, T, d_k]
```

æ‰€ä»¥ï¼š

$$
Q, K, V \in \mathbb{R}^{B \times h \times T \times d_k}
$$

è¯´æ˜ï¼š

* ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯ batch
* ç¬¬äºŒä¸ªç»´åº¦æ˜¯å¤´æ•°
* æ¯ä¸ªå¤´æœ‰è‡ªå·±çš„ä¸€ä»½ Q/K/Vï¼Œç»´åº¦æ˜¯ $T \times d_k$

---

## âœ… Step 4ï¼šScaled Dot-Product Attentionï¼ˆæ¯ä¸ªå¤´ï¼‰

å¯¹æ¯ä¸ªå¤´è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼š

$$
\text{Attention}(Q_i, K_i, V_i) = \text{softmax}\left( \frac{Q_i K_i^\top}{\sqrt{d_k}} \right) V_i
$$

æˆ‘ä»¬å¯¹æ‰€æœ‰å¤´å¹¶è¡Œåšï¼š

$$
\text{scores} = \frac{Q K^\top}{\sqrt{d_k}} \quad \text{shape: } [B, h, T, T]
$$

$$
\text{weights} = \text{softmax(scores)} \quad \text{shape: } [B, h, T, T]
$$

$$
\text{output\_per\_head} = \text{weights} \cdot V \quad \text{shape: } [B, h, T, d_k]
$$

---

## âœ… Step 5ï¼šæ‹¼æ¥å¤šä¸ªå¤´

å°†æ‰€æœ‰å¤´æ‹¼æ¥å›ä¸€ä¸ªå¤§å¼ é‡ï¼š

```python
concat = output_per_head.transpose(1, 2).reshape(B, T, h * d_k)
```

$$
\text{Concat}(\text{head}_1, \dots, \text{head}_h) \in \mathbb{R}^{B \times T \times d_{\text{model}}}
$$

å› ä¸ºï¼š

$$
h \cdot d_k = d_{\text{model}}
$$

---

## âœ… Step 6ï¼šè¾“å‡ºæŠ•å½±

å†åšä¸€æ¬¡çº¿æ€§å˜æ¢å¾—åˆ°æœ€ç»ˆè¾“å‡ºï¼š

$$
\text{MHA}(X) = \text{Concat}(\text{heads}) \cdot W^O
$$

å…¶ä¸­ï¼š

* $W^O \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$
* è¾“å‡ºå½¢çŠ¶ï¼š

  $$
  \boxed{\text{Output} \in \mathbb{R}^{B \times T \times d_{\text{model}}}}
  $$

---

## âœ… æ€»ä½“æµç¨‹å›¾ï¼ˆå¸¦å½¢çŠ¶ï¼‰

```text
Input:
  X âˆˆ [B, T, d_model]

â†“ Linear: Q = XW^Q, K = XW^K, V = XW^V
  Q, K, V âˆˆ [B, T, d_model]

â†“ Reshape + Transpose
  Q, K, V âˆˆ [B, h, T, d_k]

â†“ Scaled Dot-Product Attention (æ¯ä¸ªå¤´)
  Attention weights: [B, h, T, T]
  Head outputs:      [B, h, T, d_k]

â†“ Concatenate all heads
  [B, T, h*d_k] = [B, T, d_model]

â†“ Linear projection W^O
  Output âˆˆ [B, T, d_model]
```

---

## âœ… å°ç»“æ‰€æœ‰å…³é”®å˜é‡å’Œå½¢çŠ¶ï¼š

| åç§°                | å«ä¹‰                   | å½¢çŠ¶                                         |
| ----------------- | -------------------- | ------------------------------------------ |
| X                 | è¾“å…¥åºåˆ—                 | $B \times T \times d_{\text{model}}$       |
| W^Q/K/V           | Q/K/V æƒé‡çŸ©é˜µ           | $d_{\text{model}} \times d_{\text{model}}$ |
| Q/K/V             | query/key/value      | $B \times T \times d_{\text{model}}$       |
| åˆ†å¤´åçš„ Q/K/V        | å¤šå¤´ attention è¾“å…¥      | $B \times h \times T \times d_k$           |
| attention weights | softmax(QKáµ€ / âˆšd\_k) | $B \times h \times T \times T$             |
| head\_i           | æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›è¾“å‡º            | $B \times h \times T \times d_k$           |
| concat            | æ‰€æœ‰å¤´æ‹¼æ¥ç»“æœ              | $B \times T \times d_{\text{model}}$       |
| W^O               | è¾“å‡ºæŠ•å½±çŸ©é˜µ               | $d_{\text{model}} \times d_{\text{model}}$ |
| Output            | æœ€ç»ˆ MHA è¾“å‡º            | $B \times T \times d_{\text{model}}$       |

---

