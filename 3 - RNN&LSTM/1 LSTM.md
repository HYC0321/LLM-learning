## 📖 什么是 LSTM？

LSTM 是一种 **循环神经网络（RNN）** 的变体，专门为了解决普通 RNN 的 **长距离依赖问题**（long-term dependency）设计的。

普通 RNN 在处理长序列时会遇到 **梯度消失**（gradient vanishing）或 **梯度爆炸**（gradient exploding）的问题，导致模型记不住“很久以前”的信息。而 LSTM 通过引入一个 **“记忆单元”** 和 **门控机制**，有效地解决了这个问题。

LSTM 最早由 **Hochreiter & Schmidhuber (1997)** 提出。

---

## 🧠 LSTM 的结构

LSTM 的核心是它的 **细胞状态（Cell State）** 和 3 个“门”：

| 组件                   | 作用                       |
| -------------------- | ------------------------ |
| **Cell State**（细胞状态） | 像一条传送带，携带长期信息，通过门控调整信息流动 |
| **Forget Gate**（遗忘门） | 决定丢弃多少旧信息                |
| **Input Gate**（输入门）  | 决定接受多少新信息                |
| **Output Gate**（输出门） | 决定暴露多少信息到下一层/时间步         |

---

### 🔥 LSTM 的计算流程

假设：

* 输入：当前时间步输入 $x_t$
* 上一时间步隐藏状态 $h_{t-1}$ 和细胞状态 $C_{t-1}$
* 输出：当前隐藏状态 $h_t$ 和细胞状态 $C_t$

---

#### 1️⃣ 遗忘门（Forget Gate）

决定“哪些信息需要忘记”。
公式：

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

* $f_t$：0\~1之间的向量，1=完全保留，0=完全丢弃
* $\sigma$：Sigmoid 激活函数

---

#### 2️⃣ 输入门（Input Gate）

决定“哪些新信息写入细胞状态”。
两步：

* 生成候选值 $\tilde{C}_t$

$$
\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
$$

* 生成写入比例 $i_t$

$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
$$

---

#### 3️⃣ 更新细胞状态

综合“保留的旧信息”和“新写入信息”：

$$
C_t = f_t * C_{t-1} + i_t * \tilde{C}_t
$$

---

#### 4️⃣ 输出门（Output Gate）

决定“当前时间步暴露多少信息”：

$$
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$

$$
h_t = o_t * \tanh(C_t)
$$

---

## 👀 和普通 RNN 对比

| 特性     | RNN | LSTM      |
| ------ | --- | --------- |
| 长期依赖问题 | 严重  | 大幅缓解      |
| 结构     | 简单  | 复杂（引入门机制） |
| 参数量    | 少   | 多         |

---

## RNN/LSTM无法高效并行计算
**RNN/LSTM 无法高效并行的根本原因**就是 **序列依赖性（sequential dependency）**。

---

### 📦 具体原因：

RNN/LSTM 的计算流程中，每一个时间步 $t$ 的隐藏状态 $h_t$ 都依赖于前一个时间步的输出 $h_{t-1}$：

$$
h_t = f(h_{t-1}, x_t)
$$

这种 **前后时间步强依赖**（temporal dependency）意味着：

* 在计算 $h_t$ 之前必须先得到 $h_{t-1}$。
* 每个时间步的计算是串行的，无法同时展开。

**细胞状态 $C_t$** 在 LSTM 中也一样，它同样是：

$$
C_t = f(C_{t-1}, h_{t-1}, x_t)
$$

因此整个序列的计算只能按时间步一步步推进。

---